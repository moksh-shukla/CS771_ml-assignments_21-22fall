{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MokshShukla_180433_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeJM29LgXA-o"
      },
      "source": [
        "# This is the Assignment 2 for the course **CS771** (2021-22 I)<br>\n",
        "### ***Name*** : Moksh Shukla\n",
        "### ***Roll No.*** : 180433"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pPPVrvVwLoy"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time #to calculate time in Q1(e)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrtOeGSyxSl-"
      },
      "source": [
        "##Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8NaGLFAcxeG"
      },
      "source": [
        "Here I have slightly modified the ```gradient_descent``` function: ```x = np.add(x,delta) ``` in place of ```x += delta``` and ```np.round(x,3)``` in place of ```round(x*1000)/1000``` to accomodate the function for ```np``` arrays which makes it usable for other parts as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViFAYl-rwrB_"
      },
      "source": [
        "def gradient_descent (gradient, init_,learn_rate, n_iter=1000, tol=1e-06):\n",
        "  x = init_\n",
        "  for _ in range(n_iter):\n",
        "    delta = -learn_rate*gradient(x) \n",
        "    if np.all(np.abs(delta) <= tol):\n",
        "      break \n",
        "    x = np.add(x,delta) \n",
        "    # x+=delta\n",
        "  return np.round(x,3)\n",
        "  # return round(x*1000)/1000"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMAU-tP1xlaP"
      },
      "source": [
        "##Part(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg5HtQ42ddD0"
      },
      "source": [
        "i ) To find minima for $x^2 + 3x + 2$, I simply define a gradient function ```grad_f1``` which is passed to ```gradient_descent``` and it returns the value of $x$ at which minima occurs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjISU21-xOJi"
      },
      "source": [
        "# f1(x) = x^2 + 3x + 4\n",
        "def grad_f1(val):\n",
        "  x = val\n",
        "  f_prime = 2*x + 3\n",
        "\n",
        "  return f_prime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln7yJd-V5jff",
        "outputId": "1b25a949-2c04-4601-94d2-0e10e1338b3e"
      },
      "source": [
        "f1_min = gradient_descent(grad_f1, 1, 0.1) #initial value = 1, learning  rate = 0.1\n",
        "print(\"Minima Occurs at:\" + str(f1_min)) #returns the value at which minima occurs for the function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minima Occurs at:-1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ_n5R5od9fI"
      },
      "source": [
        "ii ) To find minima for $x^4 - 3x^2 + 2x$, I simply define a gradient function ```grad_f2``` which is passed to ```gradient_descent``` and it returns the value of $x$ at which minima occurs. This function has multiple optima so a choice of good starting point determines whether we reach a local or global optima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0Lew4Y5-1s"
      },
      "source": [
        "# f2(x) = x^4 - 3x^2 + 2x\n",
        "\n",
        "def grad_f2(val):\n",
        "  x = val\n",
        "  f_prime = 4*x*x*x-6*x+2\n",
        "\n",
        "  return f_prime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhLuAz2B6VQm",
        "outputId": "7b35ccb4-561a-4d5c-d0cf-3904e472cc9f"
      },
      "source": [
        "f2_min = gradient_descent(grad_f2, 0, 0.02) #initial value = 0, learning  rate = 0.02\n",
        "print(\"Minima Occurs at:\" + str(f2_min)) #returns the value at which minima occurs for the given function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minima Occurs at:-1.366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2fXADuF9M7H"
      },
      "source": [
        "## Part(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re7zcIFzEVQJ"
      },
      "source": [
        "We need to fit the following regression equation $$y = aX+b$$ <br>\n",
        "So, for the above equation our ***gradient descent*** update steps for $a$ and $b$ are: $$a^{t+1} = a^t - \\alpha \\frac{1}{n}\\sum_{i=1}^{n} ((y_{pred}^i - y^i)x^i)$$<br>\n",
        "$$b^{t+1} = b^t - \\alpha \\frac{1}{n}\\sum_{i=1}^{n} (y_{pred}^i - y^i)$$<br>\n",
        "So the corresponding expressions for gradients are: ```grad_a```$ =  \\frac{1}{n}\\sum_{i=1}^{n} ((y_{pred}^i - y^i)x^i)$ ; ```grad_b``` = $\\frac{1}{n}\\sum_{i=1}^{n} (y_{pred}^i - y^i)$<br><br>\n",
        "The following gradients are implemented in the function ```linearGrad``` which returns the gradients for parametres $a$ and $b$. Inside the function before calculating the gradient I always shuffle the complete data using ```np.random.choice```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH1Xxhx39PBS"
      },
      "source": [
        "# y = ax+b\n",
        "def linearGrad(param):\n",
        "  n = X.shape[0]\n",
        "  np.random.seed(0)\n",
        "  randIdx = np.random.choice(n, n) #using this I shuffle my complete data everytime\n",
        "  X_gd = X[randIdx] #shuffled X\n",
        "  y_gd = y[randIdx] #shuffled y\n",
        "  a = param[0]\n",
        "  b = param[1]\n",
        "  ynew = a*X_gd + b\n",
        "  grad_a = ((ynew-y_gd)*X_gd).mean()\n",
        "  grad_b = (ynew-y_gd).mean()\n",
        "  return np.array([grad_a, grad_b])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUsm9SES9T_V"
      },
      "source": [
        "## Part(c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zwLHif19V7s"
      },
      "source": [
        "np.random.seed(0)\n",
        "X = 2.5*np.random.randn(10000) + 1.5\n",
        "res = 1.5*np.random.randn(10000)\n",
        "y = 2 + 0.3*X + res"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXW5mJnZhcys"
      },
      "source": [
        "Here, I test the ```gradient_descent``` for our simulated data using ```linearGrad``` as our gradient calculation function. I use ```[3.5, 2.5]``` as the starting values and ```0.05``` as the optimum learning rate. As a result I get the fitted equation as $y = 0.292x+2.023$ which is very close to the ideal $y = 0.3x+2$ .<br><br>\n",
        "The code cell below also calculates the time taken by ```gradient_descent``` to run using the ```time``` module of python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbH9JVMt4S_3",
        "outputId": "7005f97c-166d-4b51-e490-87dc16b7f418"
      },
      "source": [
        "begin = time.time()\n",
        "reg_params = gradient_descent(linearGrad, init_=np.array([3.5,2.5]), learn_rate=0.05)\n",
        "print('y = {:.3f}*X + {:.3f}'.format(reg_params[0], reg_params[1]))\n",
        "end = time.time()\n",
        "print(f\"Total runtime of the program is {end - begin}\")"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 0.292*X + 2.023\n",
            "Total runtime of the program is 0.09563922882080078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwIbq-CUjVkd"
      },
      "source": [
        "## Part(d): Minibatch SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9exjAKnMibgG"
      },
      "source": [
        "Here, I test the ```gradient_descent``` for our simulated data using ```minibatchSGD_grad``` as our gradient calculation function. I use ```[3.5, 2.5]``` as the starting values and ```0.05``` as the optimum learning rate. As a result I get the fitted equation as $y = 0.301x+2.047$ (get this value when you uncomment the seed line ```np.random.seed(1)``` inside the function) which is very close to the ideal $y = 0.3x+2$ .<br><br>\n",
        "The code cell below also calculates the time taken by ```gradient_descent``` to run using the ```time``` module of python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0-2-Lz29-nt"
      },
      "source": [
        "def miniBatchSGD_grad(param):\n",
        "  n = X.shape[0]\n",
        "  #np.random.seed(1)\n",
        "  randIdx = np.random.choice(n, B) #randomly choose B numbers out of n possible values, where B = batchSize\n",
        "  X_mini = X[randIdx] #select values of X based on randomly chosen values previously\n",
        "  y_mini = y[randIdx] #select values of y based on randomly chosen values previously\n",
        "  a = param[0]\n",
        "  b = param[1]\n",
        "  ynew = a*X_mini + b\n",
        "  grad_a = ((ynew-y_mini)*X_mini).mean() #gradient of a\n",
        "  grad_b = (ynew-y_mini).mean()  #gradient of b\n",
        "  return np.array([grad_a, grad_b])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUAi6Gm2_Be6",
        "outputId": "90815f65-1303-4a6d-f4e2-af1c06079828"
      },
      "source": [
        "begin = time.time()\n",
        "B = 256 #B = batchSize\n",
        "reg_params = gradient_descent(miniBatchSGD_grad, init_= np.array([3.5,2.5]), learn_rate=0.05) #initial value = [3.5, 2.5], learning rate = 0.05\n",
        "print('y = {:.3f}*X + {:.3f}'.format(reg_params[0], reg_params[1]))\n",
        "end = time.time()\n",
        "print(f\"Total runtime of the program is {end - begin}\")"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 0.301*X + 2.047\n",
            "Total runtime of the program is 0.0227506160736084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrPDr-0ekEas"
      },
      "source": [
        "## Part(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1FnDWXuT4mq"
      },
      "source": [
        "minBatch SGD becomes SGD when Batch Size = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RGVtXaxT2Rr",
        "outputId": "12ea5e55-8118-4c61-e281-3cb70feb382a"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "begin = time.time()\n",
        "B = 1 #B = batchSize\n",
        "reg_params = gradient_descent(miniBatchSGD_grad, init_= np.array([3.5,2.5]), learn_rate=0.05) #initial value = [3.5, 2.5], learning rate = 0.05\n",
        "end = time.time()\n",
        "print(f\"Total runtime of the program is {end - begin}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total runtime of the program is 0.003300905227661133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtRHv5Z4ZF0"
      },
      "source": [
        "From Part(c), we find the runtime for ```gradient_descent``` to be ***0.096 secs***, where as for ```miniBatchSGD``` we find the runtime to be ***0.023 secs***. Thus **miniBatchSGD** performs faster than normal gradient descent, which is expected since gradient descent uses all points to calculate gradient for the update step.<br><br>\n",
        "SGD as well as miniBatch SGD both perform better in terms of time as compared to Gradient Descent which can be seen in Plot 2 below.<br><br>\n",
        "Order of Performance just on the basis of time: Gradient Descent > miniBatchSGD > SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvslG6oiVgHT"
      },
      "source": [
        "#### Computation of Optimum Mini Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RawEEgRBLeuN",
        "outputId": "7df71969-9871-42b2-9550-10db25fef056"
      },
      "source": [
        "batch_sizes = [] #creating a list of different batch sizes to test for optimum batch size\n",
        "i = 0\n",
        "while(2**i < 10000):\n",
        "  batch_sizes.append(2**i)\n",
        "  i+=1\n",
        "batch_sizes.append(10000)\n",
        "batch_sizes # using batch sizes in the powers of 2"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 10000]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qyZaJvCCQKV"
      },
      "source": [
        "loss_lst = [] #list to store training loss for all minibatch sizes\n",
        "runTime = [] #list to store total time taken for all minibatch sizes\n",
        "bIter = 7    #no. of iterations used to calculate avg loss and avg time taken for a particular batch size\n",
        "learn_rate = 0.05\n",
        "n_iter = 1000\n",
        "\n",
        "def loss(param): #loss function to calculate total loss for a particular param\n",
        "  n = X.shape[0]\n",
        "  a = param[0]\n",
        "  b = param[1]\n",
        "  ynew = a*X + b\n",
        "  regLoss = np.dot((ynew- y).T,(ynew - y))/n ## ||(ynew-y)||^2\n",
        "  return regLoss\n",
        "\n",
        "for B in batch_sizes:\n",
        "  lossCurr = 0\n",
        "  timeCurr = 0\n",
        "  for _ in range(bIter):\n",
        "    init_ =  [3.5,2.5] #initialisation for param\n",
        "    start_time = time.time()\n",
        "    param = gradient_descent(miniBatchSGD_grad,init_,learn_rate,n_iter)\n",
        "    end_time = time.time()\n",
        "    loss_temp = loss(param) #to store loss value at a single iteration\n",
        "    lossCurr+=loss_temp #stores loss value for all iterations by adding them up\n",
        "    timeCurr+=(end_time-start_time)\n",
        "\n",
        "  loss_lst.append(lossCurr/bIter)\n",
        "  runTime.append(timeCurr/bIter)  "
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu-5YtIYXxYV"
      },
      "source": [
        "#### **Plot of Training Loss v/s $\\log_2$(batch_size) for Minibatch SGD**<br>\n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Plot 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "5AlkLa8fNSj_",
        "outputId": "8ccc0872-f8eb-4f28-8095-4484f64a2c5d"
      },
      "source": [
        "plt.xlabel(\"Log_2(batch size)\")\n",
        "plt.ylabel(\"Training loss\")\n",
        "plt.plot(np.log2(batch_sizes),loss_lst,marker='o')\n",
        "plt.show()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbiimUG-YIjN"
      },
      "source": [
        "#### **Plot of Time Taken v/s $\\log_2$(batch_size) for Minibatch SGD**<br>\n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Plot 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "w4cySZ4-Nil1",
        "outputId": "205a776a-d5f3-4224-f3b8-78cf4d7f31e8"
      },
      "source": [
        "plt.xlabel(\"Log_2(batch size)\")\n",
        "plt.ylabel(\"Time taken\")\n",
        "plt.plot(np.log2(batch_sizes),runTime,marker = 'o')\n",
        "plt.show()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZpfVNXoXvQ_"
      },
      "source": [
        "### Conclusion:\n",
        "As we can see from **plot1** that with increase in batch size the training loss drops which afterwards gets saturated. This is expected as when the gradient update step sees more data, loss is bound to decrease.<br>\n",
        "We can also see from **plot2** that with increase in batch size the time taken for training increases. This is also expected since more the data points our update step sees more will be the time involved in computations.<br>\n",
        "Hence, there exists an optimum miniBatch Size.<br>\n",
        "So, our choice of **Optimum miniBatch Size** will be based on both the 2 parameters: *Training Loss* as well as *Time Taken to Train*.<br>\n",
        "Based on this when we observe both the plots we find that optimum of both the decided parameters occur when we have $\\log_2miniBatch = 8$ i.e. $miniBatchSize = 2^8 = 256$ <br><br>\n",
        "Hence, we can conclude that *approximately* optimum miniBatch Size is ***256***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBPw0fjSOMft"
      },
      "source": [
        "##  Q2 Part(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KjVgPqKPP36"
      },
      "source": [
        "Using Simple Conditional Probability:<br><br>\n",
        "$P(Cold = True, Fever = True) = P(Fever = True|Cold = True) \\times P(Cold = True)$ <br>\n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ = (0.307)\\times(0.02) = 0.00614$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MpzbecyPmXy"
      },
      "source": [
        "## Q2 Part (b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wemnqrWaTWUc"
      },
      "source": [
        "#### **We need to calculate $P(Cold=True|Cough=True)$ which is given (Using Bayes Theorem) by:** <br><br>\n",
        "$$P(Cold=True|Cough=True) = \\frac{P(Cough=True|Cold=True)\\times P(Cold=True)}{P(Cough=True)}$$<br>\n",
        "#### **To calculate the required probability we need the values of the 3 terms in our expression above. We already know $P(Cold=True)=0.02$ from the Bayesian Tables in the problem. Now first let us calculate $P(Cough=True)$**<br><br>\n",
        "#### **Applying Law of Total Probability:**\n",
        "\n",
        "Eq1.) $P(Cough=True) = P(Cough=True|Cold=True, LungDisease=True)*P(Cold=True)*P(LungDisease=True)+P(Cough=True|Cold=False, LungDisease=True)*P(Cold=False)*P(LungDisease=True)+P(Cough=True|Cold=True, LungDisease=False)*P(Cold=True)*P(LungDisease=False)+P(Cough=True|Cold=False, LungDisease=False)*P(Cold=False)*P(LungDisease=False)$<br><br>\n",
        "#### **The Required Conditional Probabilities and $P(Cold=True)$ & $P(Cold=False)$ can be found from the tables. For solving the expression above we need $P(Lung Disease=True)$ & $P(Lung Disease=False)$**<br>\n",
        "$P(LungDisease=True) = P(LungDisease=True|Smokes=True)P(Smokes=True) + P(LungDisease=True|Smokes=False)P(Smokes=False)$<br><br>\n",
        "$P(LungDisease=True) = 0.1009*0.2+0.001*0.8 = 0.02098$<br><br>\n",
        "$P(LungDisease=False)=1-P(LungDisease=True)$<br>\n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;$=1-0.02098 = 0.9702$<br><br>\n",
        "#### **Now Plugging in the values in *Eq. 1* of $P(Cough=True)$ we get:**\n",
        "\n",
        "$P(Cough=True) = 0.7525*0.02*0.02098+0.505*0.98*0.02098+0.505*0.02*0.9702+$<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;$0.01*0.98*0.97902$<br>\n",
        "$P(Cough=True) = 0.0302$<br><br>\n",
        "\n",
        "#### **Now let us calculate the value of $P(Cough=True|Cold=True)$:**<br>\n",
        "$P(Cough=True|Cold=True) = P(Cough=True|Cold=True, LungDisease=True)*P(LungDisease=True) + P(Cough=True|Cold=True, LungDisease=False)*P(LungDisease=False)$<br><br>\n",
        "$P(Cough=True|Cold=True) = 0.7525*0.02098+0.505*0.97902$<br>\n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;$=0.5102$<br>\n",
        "<br>\n",
        "#### **Finally calculating the required probability**\n",
        "$P(Cold=True|Cough=True) = \\frac{P(Cough=True|Cold=True)\\times P(Cold=True)}{P(Cough=True)}$<br><br>\n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;$=\\frac{0.5102\\times0.02}{0.0302} = 0.3381$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvM1upBIbeoQ"
      },
      "source": [
        "## Q3: Finding MLE for multinomial distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvQRwEAGKmYh"
      },
      "source": [
        "Let $X$ be a Random Variable following multinomial distribution in which $x_i$ is the number of success of the $i^{th}$ category for $n$ random draws, and $p_i$ is the probability of success of the $i^{th}$ category. Then, $$P(X=x, n, \\mathbf{p}) = n! \\prod_{i=1}^{k} \\frac{p_{i}^{x_i}}{x_i !}$$<br>\n",
        "where, <br> \n",
        "$$\\sum_{i=1}^{k} p_i = 1$$ <br>\n",
        "For the given multinomial distribution log-likelihood can be written as:$$l(\\mathbf{p}) = \\log(n! \\prod_{i=1}^{k} \\frac{p_{i}^{x_i}}{x_i !})$$<br>\n",
        "$$l(\\mathbf{p})=\\log n! + \\sum_{i=1}^{k} x_i \\log p_i - \\sum_{i=1}^{k} \\log x_i!$$<br>\n",
        "\n",
        "For MLE estimate of $\\mathbf{p}$, assuming $n$ is known, we solve the following optimization problem:$$\\max_{\\mathbf{p}}l(\\mathbf{p},n)$$ <br>\n",
        "such that $$\\sum_{k=1}^{K}p_i = 1$$\n",
        "\n",
        "Maximizing the log-likelihood is a constrained optimization problem and we use *Lagrangian* to solve this.$$\\mathcal{L}(\\mathbf{p},\\lambda) = l(\\mathbf{p}) + \\lambda(1-\\sum_{i=1}^{k} p_i)$$<br>\n",
        "Solving: $$\\frac{\\partial \\mathcal{L}}{\\partial p_i} = 0$$<br>\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} = \\frac{\\partial l}{\\partial p_i}+\\frac{\\partial (\\lambda(1-\\sum_{i=1}^{k} p_i))}{\\partial p_i} = 0$$<br>\n",
        "$$\\frac{\\partial l}{\\partial p_i} = 0 + \\frac{\\partial \\sum_{i=1}^{k} x_i \\log p_i }{\\partial p_i} + 0$$<br>\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} = \\frac{\\partial \\sum_{i=1}^{k} x_i \\log p_i }{\\partial p_i} - \\lambda\\partial\\frac{ \\sum_{i=1}^{k} p_i }{\\partial p_i} = 0$$<br>\n",
        "$$ \\frac{x_i}{p_i} - \\lambda = 0 $$<br>\n",
        "$$p_i = \\frac{x_i}{\\lambda}$$<br>\n",
        "Since given our constraint, $$\\sum_{i=1}^{k} p_i = 1$$<br>\n",
        "we have, $$\\sum_{i=1}^{k} \\frac{x_i}{\\lambda} = 1$$<br>\n",
        "$$\\lambda = \\sum_{i=1}^{k} x_i$$<br>\n",
        "$$\\lambda = n$$<br>\n",
        "Therefore our MLE estimate:$$\\hat{p_i} = \\frac{x_i}{n}$$<br>\n",
        "$$\\hat{\\mathbf{p}} = [\\frac{x_1}{n}, \\frac{x_2}{n}....\\frac{x_k}{n}]$$\n",
        "\n"
      ]
    }
  ]
}